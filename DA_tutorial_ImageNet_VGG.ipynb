{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Defensive_approximation_ImageNet_VGG.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AG-X09/Defensive-Approximation/blob/main/Defensive_approximation_ImageNet_VGG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZY9mUp8abadG",
        "outputId": "44a93d2f-60cf-4515-c63a-88e96983d1e6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkBsXdsxbjVI",
        "outputId": "f690357f-530a-4aab-b675-ae3f87105f9b"
      },
      "source": [
        "cd /content/gdrive/MyDrive/"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALStrxxFGFKk"
      },
      "source": [
        "# Approximate BFloat16 multiplier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNVFQopyFwz2"
      },
      "source": [
        "#Approximate 4x4 array multiplier (using the approximate mirror adder AMA5)\n",
        "def appx_multiplier4x4_AMA5(A,B):\n",
        "\n",
        "    S = 0\n",
        "    if (A == 0) or (B == 0):\n",
        "        S = 0\n",
        "    elif (A == 1):\n",
        "        S = B\n",
        "    elif (A % 2 == 0) & (A>1):\n",
        "        if (B < 8): \n",
        "            S = 0\n",
        "        else:\n",
        "            S = 32 * (A/2)\n",
        "    else:\n",
        "        if (B < 8): \n",
        "            S = B\n",
        "        else:\n",
        "            S = B + 32 * (A-1)/2\n",
        "    return S  "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "756A-hBOGN_I"
      },
      "source": [
        "#Approximate 8x8 array multiplier\n",
        "def appx_multiplier8x8(a,b):\n",
        "\n",
        "    a0b0 = appx_multiplier4x4_AMA5(int(a[4:8],2), int(b[4:8],2))\n",
        "    a1b0 = appx_multiplier4x4_AMA5(int(a[0:4],2), int(b[4:8],2))\n",
        "    a0b1 = appx_multiplier4x4_AMA5(int(a[4:8],2), int(b[0:4],2))\n",
        "    a1b1 = appx_multiplier4x4_AMA5(int(a[0:4],2), int(b[0:4],2))\n",
        "    S = (a0b0 + (a1b0 + a0b1)*16 + a1b1*256) \n",
        "    S = format(int(S), '016b')\n",
        "    return S"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ub-JrY-1GVhl"
      },
      "source": [
        "#Approximate BFloat16 multiplier\n",
        "def BF_appx_mul(A,B):\n",
        "\n",
        "  if (abs(A)<1e-36) or (abs(B)<1e-36) or (A == 0) or (B==0):\n",
        "    s = 0\n",
        "  else:\n",
        "    S = ['0','00000000','00000000000000000000000']\n",
        "\n",
        "    a = ''.join(bin(c).replace('0b', '').rjust(8, '0') for c in struct.pack('!f', A))  \n",
        "    b = ''.join(bin(c).replace('0b', '').rjust(8, '0') for c in struct.pack('!f', B)) \n",
        "\n",
        "    sign_ab = int(a[0])^int(b[0])\n",
        "\n",
        "    exponent_ab = int(a[1:9],2) + int(b[1:9],2) - 127\n",
        "    if exponent_ab<1:\n",
        "        s = 0\n",
        "    else:\n",
        "        mantissa_ab = appx_multiplier8x8('1'+a[9:16],'1'+b[9:16])\n",
        "  \n",
        "        if mantissa_ab[0] == '1':\n",
        "            final_mantissa = mantissa_ab[1:8]\n",
        "            exponent_ab = exponent_ab + 1\n",
        "        else:\n",
        "            final_mantissa = mantissa_ab[2:9] \n",
        "        S = [\"{}\".format(sign_ab), format(exponent_ab,'08b'), final_mantissa +'0000000000000000']\n",
        "        S = ''.join(S) \n",
        "        s = struct.unpack('!f',struct.pack('!I', int(S, 2)))[0]\n",
        "\n",
        "  return s "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyFf6PqUGyqg"
      },
      "source": [
        "from torchvision import models, datasets, transforms\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "VAL_SIZE = 50000\n",
        "\n",
        "\n",
        "device = torch.device( \"cpu\")\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from joblib import Parallel, delayed"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2TRxFgEHEw7"
      },
      "source": [
        "# Approximate Convolution Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SPV6WLRHMQT"
      },
      "source": [
        "class convAppx(torch.autograd.Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, X, weight, bias, padding, stride):\n",
        "\n",
        "        ctx.save_for_backward(X, weight, bias)        \n",
        "        (m, n_C_prev, n_H_prev, n_W_prev) = X.shape\n",
        "        (n_C, n_C_prev, f, f) = weight.shape\n",
        "\n",
        "        n_H = ((n_H_prev - f + 2 * padding[0]) // stride[0]) + 1\n",
        "        n_W = ((n_W_prev - f + 2 * padding[0]) // stride[0]) + 1\n",
        "\n",
        "\n",
        "\n",
        "        def appx_mul(A,B):\n",
        "            window = np.zeros((A.shape))\n",
        "            for l in range(A.shape[0]):\n",
        "              for j in range(A.shape[1]):\n",
        "                for k in range(A.shape[2]):\n",
        "                  window[l,j,k] = BF_appx_mul(A[l,j,k],B[l,j,k])  #A[l,j,k]*B[l,j,k]\n",
        "            return np.sum(window)\n",
        "\n",
        "        def mul_channel( weight,bias, x_pad, n_H, n_W,f):\n",
        "              Z = np.zeros(( n_H, n_W ))\n",
        "              for h in range(n_H):\n",
        "                  for w in range(n_W):\n",
        "                      vert_start = h\n",
        "                      vert_end = vert_start + f\n",
        "                      horiz_start = w\n",
        "                      horiz_end = horiz_start + f\n",
        "            \n",
        "                      x_slice = x_pad[:, vert_start:vert_end, horiz_start:horiz_end]  \n",
        "                      Z[ h, w] = appx_mul(x_slice,weight)  #torch.matmul(A,B)\n",
        "                      Z[ h, w] += bias\n",
        "              return Z\n",
        "      \n",
        "        X_pad = F.pad(X, (padding[0],padding[0],padding[0],padding[0]))\n",
        "        weight = weight.data.numpy()\n",
        "        bias = bias.data.numpy()\n",
        "        X_pad = X_pad.data.numpy()\n",
        "\n",
        "        Z = np.zeros((m, n_C, n_H, n_W ))     \n",
        "        import os\n",
        "        from joblib import dump, load\n",
        "        \n",
        "        folder = './joblib_memmap'\n",
        "        try:\n",
        "            os.mkdir(folder)\n",
        "        except FileExistsError:\n",
        "            pass\n",
        "        data_ = X_pad[0]\n",
        "        \n",
        "        data_filename_memmap = os.path.join(folder, 'data___memmap')\n",
        "        dump(data_, data_filename_memmap)\n",
        "        data_ = load(data_filename_memmap, mmap_mode='r')         \n",
        "\n",
        "    \n",
        "        for i in range(m):\n",
        "            for c in range(n_C):\n",
        "                Z[i,c] = mul_channel( weight[c, :, :, :],bias[c], data_, n_H, n_W, f)       \n",
        "            #To use Joblib Parallel uncomment this          \n",
        "            #x_pad = X[0]\n",
        "            #Z[0] = Parallel(n_jobs=8)(delayed(mul_channel)( weight[c, :, :, :],bias[c], X_pad[0], n_H, n_W, f)  for c in  range(n_C) )  \n",
        "        return torch.from_numpy(Z).float() \n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        x, weight, bias = ctx.saved_tensors \n",
        "\n",
        "        grad_input = grad_weight = grad_bias = None\n",
        "\n",
        "\n",
        "        def convolutionBackward(dconv_prev, conv_in, weight, padding =1, stride=1):\n",
        "            (m, n_C_prev, n_H_prev, n_W_prev) = conv_in.shape\n",
        "            (n_C, n_C_prev, f, f) = weight.shape\n",
        "            (m, n_C, n_H, n_W) = dconv_prev.shape\n",
        "\n",
        "            dA_prev = torch.zeros((m, n_C_prev, n_H_prev, n_W_prev))\n",
        "            dW = torch.zeros((n_C, n_C_prev, f, f))\n",
        "            db = torch.zeros((n_C))\n",
        "            X_pad = F.pad(conv_in, (padding,padding,padding,padding))\n",
        "            dA_prev_pad = F.pad(dA_prev, (padding,padding,padding,padding))\n",
        "\n",
        "            for i in range(m):\n",
        "                x_pad = X_pad[i]\n",
        "                da_prev_pad = dA_prev_pad[i]\n",
        "              \n",
        "                for c in range(n_C):\n",
        "                    for h in range(n_H):\n",
        "                        for w in range(n_W):\n",
        "                            vert_start = h + h * (stride - 1)\n",
        "                            vert_end = vert_start + f\n",
        "                            horiz_start = w + w * (stride - 1)\n",
        "                            horiz_end = horiz_start + f\n",
        "\n",
        "                            x_slice = x_pad[:, vert_start:vert_end, horiz_start:horiz_end]\n",
        "\n",
        "                            da_prev_pad[:, vert_start:vert_end, horiz_start:horiz_end] += weight[c, :, :, :] * dconv_prev[i, c, h, w]\n",
        "                        \n",
        "                            dW[c,:,:,:] += x_slice * dconv_prev[i, c, h, w]\n",
        "                            \n",
        "                            db[c] += dconv_prev[i, c, h, w]  \n",
        "                if padding == 0:\n",
        "                  dA_prev[i, :, :, :] = da_prev_pad[:]\n",
        "                else:\n",
        "                  dA_prev[i, :, :, :] = da_prev_pad[:, padding:-padding, padding:-padding] \n",
        "            return dA_prev, dW, db\n",
        "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
        "        grad_input, grad_weight, grad_bias = convolutionBackward(grad_output, x, weight)\n",
        "        grad_bias = grad_bias.squeeze()\n",
        "        return grad_input, grad_weight, grad_bias, None,None   \n",
        "\n",
        "class MyConv2d(nn.Module):\n",
        "    def __init__(self, n_channels, out_channels, kernel_size , padding, stride,dilation=1):\n",
        "        super(MyConv2d, self).__init__()\n",
        "\n",
        "        self.kernel_size = (kernel_size, kernel_size)\n",
        "        self.kernal_size_number = kernel_size * kernel_size\n",
        "        self.out_channels = out_channels\n",
        "        self.dilation = (dilation, dilation)\n",
        "        self.padding = (padding, padding)\n",
        "        self.stride = (stride, stride)\n",
        "        self.n_channels = n_channels\n",
        "        self.weight = nn.Parameter(torch.rand(self.out_channels, self.n_channels, self.kernel_size[0] , self.kernel_size[1] ))\n",
        "        self.bias = nn.Parameter(torch.rand(self.out_channels))\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = convAppx.apply(x, self.weight, self.bias, self.padding, self.stride)\n",
        "\n",
        "        return res"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ik7xF-I2SawX"
      },
      "source": [
        "Approximate Fully Connected Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EX5qHt7oHreQ"
      },
      "source": [
        "class linear_appx(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, weight, bias):\n",
        "        ctx.save_for_backward(input, weight, bias)\n",
        "\n",
        "        input = input.data.numpy()\n",
        "        weight = weight.data.numpy()\n",
        "        bias = bias.data.numpy()\n",
        "        def appx_mul(A,B):\n",
        "          window = np.zeros((A.shape[0],B.shape[1] ))\n",
        "          for k in range(A.shape[0]):\n",
        "            for l in range(B.shape[1]):\n",
        "              for j in range(A.shape[1]):\n",
        "                  window[k,l] += BF_appx_mul(A[k,j],B[j,l])  #A[k,j]*B[j,l]  #\n",
        "          return window\n",
        "\n",
        "        output = appx_mul(input,np.transpose(weight)) + bias\n",
        "        #print(output)\n",
        "        return torch.from_numpy(output).float()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        input, weight, bias = ctx.saved_tensors\n",
        "\n",
        "        grad_input = grad_output.mm(weight.float())\n",
        "        grad_weight = grad_output.t().mm(input.float())\n",
        "        grad_bias = grad_output.sum(0)\n",
        "        return grad_input, grad_weight, grad_bias\n",
        "\n",
        "\n",
        "class MyLinear(nn.Module):\n",
        "    def __init__(self,in_features, out_features ):\n",
        "        super(MyLinear, self).__init__()\n",
        "        self.fn = linear_appx.apply\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = torch.nn.Parameter(torch.randn(out_features, in_features))\n",
        "        self.bias = torch.nn.Parameter(torch.randn(out_features))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fn(x, self.weight, self.bias)\n",
        "        return x"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQ3Va3slIVda"
      },
      "source": [
        "\n",
        "try:\n",
        "    from torch.hub import load_state_dict_from_url\n",
        "except ImportError:\n",
        "    from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "#from .utils import load_state_dict_from_url\n",
        "from typing import Union, List, Dict, Any, cast\n",
        "\n",
        "\n",
        "__all__ = [\n",
        "    'VGG', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn',\n",
        "    'vgg19_bn_appx', 'vgg19_appx',\n",
        "]\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'vgg11': 'https://download.pytorch.org/models/vgg11-bbd30ac9.pth',\n",
        "    'vgg13': 'https://download.pytorch.org/models/vgg13-c768596a.pth',\n",
        "    'vgg16': 'https://download.pytorch.org/models/vgg16-397923af.pth',\n",
        "    'vgg19': 'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth',\n",
        "    'vgg11_bn': 'https://download.pytorch.org/models/vgg11_bn-6002323d.pth',\n",
        "    'vgg13_bn': 'https://download.pytorch.org/models/vgg13_bn-abd245e5.pth',\n",
        "    'vgg16_bn': 'https://download.pytorch.org/models/vgg16_bn-6c64b313.pth',\n",
        "    'vgg19_bn': 'https://download.pytorch.org/models/vgg19_bn-c79401a0.pth',\n",
        "}\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhdUO6yBJL4I"
      },
      "source": [
        "# Define Exact VGG Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttiDMPBfI5bM"
      },
      "source": [
        "\n",
        "class VGG(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        features: nn.Module,\n",
        "        num_classes: int = 1000,\n",
        "        init_weights: bool = False\n",
        "    ) -> None:\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = features\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "        if init_weights:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self) -> None:\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "def make_layers_exact(cfg: List[Union[str, int]], batch_norm: bool = False) -> nn.Sequential:\n",
        "    layers: List[nn.Module] = []\n",
        "    in_channels = 3\n",
        "    for v in cfg:\n",
        "        if v == 'M':\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "        else:\n",
        "            v = cast(int, v)\n",
        "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
        "            if batch_norm:\n",
        "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
        "            else:\n",
        "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
        "            in_channels = v\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "def _vgg_(arch: str, cfg: str, batch_norm: bool, pretrained: bool, progress: bool, **kwargs: Any) -> VGG:\n",
        "    if pretrained:\n",
        "        kwargs['init_weights'] = False\n",
        "    model = VGG(make_layers_exact(cfgs[cfg], batch_norm=batch_norm), **kwargs)\n",
        "    if pretrained:\n",
        "        #state_dict = torch.load('vgg11-bbd30ac9.pth')\n",
        "        #model.load_state_dict(state_dict)\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['vgg11']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def vgg11(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> VGG:\n",
        "    r\"\"\"VGG 11-layer model (configuration \"A\") from\n",
        "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`._\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _vgg_('vgg11', 'A', False, pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def vgg11_bn(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> VGG:\n",
        "    r\"\"\"VGG 11-layer model (configuration \"A\") with batch normalization\n",
        "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`._\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _vgg_('vgg11_bn', 'A', True, pretrained, progress, **kwargs)\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klmuEihuJWlG"
      },
      "source": [
        "# Define Approximate VGG Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD5BfLWOIn5q"
      },
      "source": [
        "import torch.utils.model_zoo as model_zoo\n",
        "class VGG_appx(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        features: nn.Module,\n",
        "        num_classes: int = 1000,\n",
        "        init_weights: bool = True\n",
        "    ) -> None:\n",
        "        super(VGG_appx, self).__init__()\n",
        "        self.features = features\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
        "        self.classifier = nn.Sequential(\n",
        "            MyLinear(512 * 7 * 7, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            MyLinear(4096, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            MyLinear(4096, num_classes),\n",
        "        )\n",
        "        if init_weights:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.features(x)\n",
        "        print(\"done!\")\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self) -> None:\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "def make_layers(cfg: List[Union[str, int]], batch_norm: bool = False) -> nn.Sequential:\n",
        "    layers: List[nn.Module] = []\n",
        "    in_channels = 3\n",
        "    for v in cfg:\n",
        "        if v == 'M':\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "        else:\n",
        "            v = cast(int, v)\n",
        "            conv2d = MyConv2d(in_channels, v, 3,1,1)\n",
        "            if batch_norm:\n",
        "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
        "            else:\n",
        "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
        "            in_channels = v\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "cfgs: Dict[str, List[Union[str, int]]] = {\n",
        "    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "\n",
        "def _vgg(arch: str, cfg: str, batch_norm: bool, pretrained: bool, progress: bool, **kwargs: Any) -> VGG_appx:\n",
        "    if pretrained:\n",
        "        kwargs['init_weights'] = False\n",
        "    model = VGG_appx(make_layers(cfgs[cfg], batch_norm=batch_norm), **kwargs)\n",
        "    if pretrained:\n",
        "        #state_dict = torch.load('vgg11-bbd30ac9.pth')\n",
        "        #model.load_state_dict(state_dict)\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['vgg11']))\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def vgg11_appx(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> VGG_appx:\n",
        "    r\"\"\"VGG 19-layer model (configuration \"E\")\n",
        "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`._\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _vgg('vgg11', 'A', False, pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def vgg11_bn_appx(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> VGG_appx:\n",
        "    r\"\"\"VGG 19-layer model (configuration 'E') with batch normalization\n",
        "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`._\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _vgg('vgg11_bn', 'A', True, pretrained, progress, **kwargs)\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHx5qER_JbuR"
      },
      "source": [
        "#PATH = 'vgg11-bbd30ac9.pth' \n",
        "model_name = \"vgg11\"\n",
        "model = vgg11(pretrained= True, progress = True)\n",
        "\n",
        "model.load_state_dict(model_zoo.load_url(model_urls['vgg11']))\n",
        "model.eval()\n",
        "# Send the model to GPU/CPU\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "model_name = \"vgg11_appx\"\n",
        "model_appx = vgg11_appx(pretrained= True, progress = True)\n",
        "\n",
        "model_appx.load_state_dict(model_zoo.load_url(model_urls['vgg11']))\n",
        "model_appx.eval()\n",
        "# Send the model to GPU/CPU\n",
        "model_appx = model_appx.to(device)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38jR7lk6KQtp"
      },
      "source": [
        "# Create shaffeled 1000 sample\n",
        "batch_size = 1000\n",
        "dataloaders = {}\n",
        "#for img_size in [224, 299]:\n",
        "\n",
        "img_size = 224\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(img_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "val_dataset = datasets.ImageFolder(root=os.path.join('./val/'), transform=val_transform)  #Check \"ImageNet_validation_data_extraction.ipynb\" to download and extract ImageNet validation data\n",
        "\n",
        "dataloaders[img_size] = torch.utils.data.DataLoader(\n",
        "    val_dataset, \n",
        "    batch_size=batch_size,                                             \n",
        "    shuffle=False, \n",
        "    num_workers= max(1, multiprocessing.cpu_count() - 2),\n",
        ")\n",
        "        \n",
        "loader = dataloaders[img_size]\n",
        "with torch.set_grad_enabled(False):\n",
        "    for i, (x_val, y_val) in enumerate(loader):\n",
        "      x_val, y_val = x_val, y_val\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwoOqWLnKbea"
      },
      "source": [
        "i = 228\n",
        "x = x_val[i].unsqueeze(0)\n",
        "y = y_val[i].unsqueeze(0) - 1\n",
        "\n",
        "out_exact = torch.nn.functional.softmax(model(x), dim=1)\n",
        "print(\"Prediction exact:\",out_exact.argmax(1))\n",
        "\n",
        "out_appx = torch.nn.functional.softmax(model_appx(x), dim=1)\n",
        "print(\"Prediction appx:\",out_appx.argmax(1))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
